{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d609da-e3e8-4aa4-8bf7-ca1c43ff11e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard loss: 11.942072868347168\n",
      "standard max gpu memory: 6524.65 MB\n",
      "Memory-Efficient loss: 11.942071914672852\n",
      "Memory-Efficient max gpu memory: 2068.77 MB\n",
      "VRAM reduction: 68.3%\n",
      "Standard GRPO Loss: 0.2337828278541565\n",
      "Standard GRPO max gpu memory: 13073.07 MB\n",
      "Memory-Efficient GRPO Loss: 0.2337827980518341\n",
      "Memory-Efficient GRPO max gpu memory: 4151.37 MB\n",
      "GRPO VRAM reduction: 68.2%\n",
      "Classic loss: 12.17023754119873\n",
      "Classic max gpu memory: 32230.35 MB\n",
      "Chunked loss: 12.170235633850098\n",
      "Chunked max gpu memory: 27760.38 MB\n",
      "VRAM reduction: 13.9%\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Tuple, Union, Unpack\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import LlamaPreTrainedModel, GenerationMixin, LlamaModel\n",
    "from transformers.models.llama.configuration_llama import LlamaConfig\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM, KwargsForCausalLM, LLAMA_INPUTS_DOCSTRING, _CONFIG_FOR_DOC\n",
    "from transformers import Cache\n",
    "from transformers.utils import replace_return_docstrings, add_start_docstrings_to_model_forward\n",
    "from transformers.utils.deprecation import deprecate_kwarg\n",
    "from transformers import AutoConfig\n",
    "\n",
    "def transformation_function(batch, linear, labels):\n",
    "    x = linear(batch).float()\n",
    "    return F.cross_entropy(x.view(-1, x.shape[-1]), labels.view(-1))\n",
    "\n",
    "class MemoryEfficientLinear(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, linear, labels, forward_function, chunk_size):\n",
    "        ctx.save_for_backward(X, labels)\n",
    "        ctx.linear = linear\n",
    "        ctx.forward_function = forward_function\n",
    "        ctx.chunk_size = chunk_size\n",
    "\n",
    "        total_output = 0.0\n",
    "        n = X.shape[0]\n",
    "        for i in range(0, n, chunk_size):\n",
    "            end_idx = min(i + chunk_size, n)\n",
    "            batch_chunk = X[i:end_idx]\n",
    "            labels_chunk = labels[i:end_idx]\n",
    "            output_chunk = forward_function(batch_chunk, linear, labels_chunk)\n",
    "            total_output += output_chunk * (end_idx - i) / n\n",
    "        return total_output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        X, labels = ctx.saved_tensors\n",
    "        linear = ctx.linear\n",
    "        forward_function = ctx.forward_function\n",
    "        chunk_size = ctx.chunk_size\n",
    "\n",
    "        grad_X = torch.zeros_like(X)\n",
    "        n = X.shape[0]\n",
    "        for i in range(0, n, chunk_size):\n",
    "            end_idx = min(i + chunk_size, n)\n",
    "            batch_chunk = X[i:end_idx].detach().requires_grad_(True)\n",
    "            labels_chunk = labels[i:end_idx]\n",
    "\n",
    "            with torch.enable_grad():\n",
    "                output_chunk = forward_function(batch_chunk, linear, labels_chunk)\n",
    "\n",
    "            chunk_grad = torch.autograd.grad(\n",
    "                output_chunk, batch_chunk,\n",
    "                grad_output * (end_idx - i) / n\n",
    "            )[0]\n",
    "            grad_X[i:end_idx] = chunk_grad\n",
    "        return grad_X, None, None, None, None\n",
    "\n",
    "def memory_efficient_linear(X, linear, labels, forward_function=transformation_function, chunk_size=1):\n",
    "    return MemoryEfficientLinear.apply(X, linear, labels, forward_function, chunk_size)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "batch_size = 4\n",
    "seq_len = 1024\n",
    "hidden_dim = 1024\n",
    "vocab_size = 128000\n",
    "\n",
    "linear = torch.nn.Linear(hidden_dim, vocab_size).to(device)\n",
    "\n",
    "X = torch.randn(batch_size, seq_len, hidden_dim, requires_grad=True, device=device)\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "loss_std = transformation_function(X, linear, labels)\n",
    "loss_std.backward()\n",
    "std_mem = torch.cuda.max_memory_allocated(device)\n",
    "print(\"standard loss:\", loss_std.item())\n",
    "print(\"standard max gpu memory: {:.2f} MB\".format(std_mem / (1024 * 1024)))\n",
    "\n",
    "linear.zero_grad()\n",
    "if X.grad is not None:\n",
    "    X.grad.zero_()\n",
    "\n",
    "### CHUNK SIZE 1 means 1 batch once now not chunk amount, we have 4 batches, standard one will process all at once\n",
    "### but ours will process one batch at once + 1 store of loss float so in theory\n",
    "### it guaranteed %75ish vram reduction, but in practice its like %50+ as shown below\n",
    "### if it does not do %50 you can do more batches to get benefit like 6 or 8\n",
    "chunk_size = 1\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "loss_chunked = memory_efficient_linear(X, linear, labels, forward_function=transformation_function, chunk_size=chunk_size)\n",
    "loss_chunked.backward()\n",
    "chunked_mem = torch.cuda.max_memory_allocated(device)\n",
    "print(\"Memory-Efficient loss:\", loss_chunked.item())\n",
    "print(\"Memory-Efficient max gpu memory: {:.2f} MB\".format(chunked_mem / (1024 * 1024)))\n",
    "\n",
    "reduction_pct = (std_mem - chunked_mem) / std_mem * 100\n",
    "print(\"VRAM reduction: {:.1f}%\".format(reduction_pct))\n",
    "\n",
    "def grpo_compute_loss(old_logits, new_logits, input_ids, mask, beta, advantages):\n",
    "    old_logits = old_logits.to(torch.float32)\n",
    "    new_logits = new_logits.to(torch.float32)\n",
    "    input_ids  = input_ids.unsqueeze(-1)\n",
    "\n",
    "    old_x = torch.gather(old_logits, dim = -1, index = input_ids).squeeze(-1)\n",
    "    new_x = torch.gather(new_logits, dim = -1, index = input_ids).squeeze(-1)\n",
    "\n",
    "    old_logsumexp = torch.logsumexp(old_logits, dim=-1)\n",
    "    old = old_x - old_logsumexp\n",
    "\n",
    "    new_logsumexp = torch.logsumexp(new_logits, dim=-1)\n",
    "    new = new_x - new_logsumexp\n",
    "\n",
    "    kl_i = torch.exp(old - new) - (old - new) - 1.0\n",
    "\n",
    "    loss_i = torch.exp(new - new.detach()) * advantages.unsqueeze(1)\n",
    "    loss_i = -(loss_i - beta * kl_i)\n",
    "\n",
    "    mask = mask.to(torch.float32)\n",
    "    n_mask_per_reward = mask.sum(1)\n",
    "\n",
    "    loss = (loss_i * mask).sum() / mask.sum()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        completion_length = n_mask_per_reward.mean()\n",
    "        mean_kl_per_reward = (kl_i * mask).sum(1) / n_mask_per_reward\n",
    "        mean_kl = mean_kl_per_reward.mean()\n",
    "    return loss, completion_length, mean_kl\n",
    "\n",
    "def grpo_accumulated_loss(\n",
    "    old_hidden_states,\n",
    "    new_hidden_states,\n",
    "    lm_head,\n",
    "    input_ids,\n",
    "    logits_to_keep,\n",
    "    completion_mask,\n",
    "    beta,\n",
    "    advantages,\n",
    "):\n",
    "    os.environ[\"UNSLOTH_RETURN_HIDDEN_STATES\"] = \"1\"\n",
    "    \n",
    "    completion_input_ids = input_ids[:, -logits_to_keep:]\n",
    "    old_hidden_states = old_hidden_states[:, -logits_to_keep-1:]\n",
    "    new_hidden_states = new_hidden_states[:, -logits_to_keep-1:]\n",
    "    new_logits = torch.matmul(new_hidden_states, lm_head.weight.t())\n",
    "    new_logits = new_logits[:, :-1, :]\n",
    "    old_logits = torch.matmul(old_hidden_states, lm_head.weight.t())\n",
    "    old_logits = old_logits[:, :-1, :]\n",
    "    loss, completion_length, mean_kl = grpo_compute_loss(\n",
    "        old_logits, new_logits, completion_input_ids, completion_mask, beta, advantages,\n",
    "    )\n",
    "    return loss, completion_length, mean_kl\n",
    "\n",
    "\n",
    "# Memory efficient GRPO implementation using batch chunking similar to MemoryEfficientLinear\n",
    "class MemoryEfficientGRPO(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, old_hidden_states, new_hidden_states, lm_head, input_ids, logits_to_keep, completion_mask, beta, advantages, chunk_size):\n",
    "        ctx.save_for_backward(old_hidden_states, new_hidden_states, input_ids, completion_mask, advantages)\n",
    "        ctx.lm_head = lm_head\n",
    "        ctx.logits_to_keep = logits_to_keep\n",
    "        ctx.beta = beta\n",
    "        ctx.chunk_size = chunk_size\n",
    "\n",
    "        bsz = input_ids.shape[0]\n",
    "        total_loss = 0.0\n",
    "        total_completion_length = 0.0\n",
    "        total_mean_kl = 0.0\n",
    "        for i in range(0, bsz, chunk_size):\n",
    "            end_idx = min(i + chunk_size, bsz)\n",
    "            old_h_chunk = old_hidden_states[i:end_idx][:, -logits_to_keep-1:]\n",
    "            new_h_chunk = new_hidden_states[i:end_idx][:, -logits_to_keep-1:]\n",
    "            input_ids_chunk = input_ids[i:end_idx]\n",
    "            completion_input_ids = input_ids_chunk[:, -logits_to_keep:]\n",
    "            comp_mask_chunk = completion_mask[i:end_idx]\n",
    "            adv_chunk = advantages[i:end_idx]\n",
    "\n",
    "            new_logits_chunk = torch.matmul(new_h_chunk, lm_head.weight.t())[:, :-1, :]\n",
    "            old_logits_chunk = torch.matmul(old_h_chunk, lm_head.weight.t())[:, :-1, :]\n",
    "            loss_chunk, completion_length_chunk, mean_kl_chunk = grpo_compute_loss(\n",
    "                old_logits_chunk, new_logits_chunk, completion_input_ids, comp_mask_chunk, beta, adv_chunk\n",
    "            )\n",
    "            weight = (end_idx - i) / bsz\n",
    "            total_loss += loss_chunk * weight\n",
    "            total_completion_length += completion_length_chunk * weight\n",
    "            total_mean_kl += mean_kl_chunk * weight\n",
    "        return total_loss, total_completion_length, total_mean_kl\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_loss, grad_completion_length, grad_mean_kl):\n",
    "        old_hidden_states, new_hidden_states, input_ids, completion_mask, advantages = ctx.saved_tensors\n",
    "        lm_head = ctx.lm_head\n",
    "        logits_to_keep = ctx.logits_to_keep\n",
    "        beta = ctx.beta\n",
    "        chunk_size = ctx.chunk_size\n",
    "        bsz = input_ids.shape[0]\n",
    "        grad_old = torch.zeros_like(old_hidden_states)\n",
    "        grad_new = torch.zeros_like(new_hidden_states)\n",
    "        for i in range(0, bsz, chunk_size):\n",
    "            end_idx = min(i + chunk_size, bsz)\n",
    "            old_h_chunk = old_hidden_states[i:end_idx].detach().requires_grad_(True)\n",
    "            new_h_chunk = new_hidden_states[i:end_idx].detach().requires_grad_(True)\n",
    "            input_ids_chunk = input_ids[i:end_idx]\n",
    "            comp_mask_chunk = completion_mask[i:end_idx]\n",
    "            adv_chunk = advantages[i:end_idx]\n",
    "            with torch.enable_grad():\n",
    "                old_h_chunk_slice = old_h_chunk[:, -logits_to_keep-1:]\n",
    "                new_h_chunk_slice = new_h_chunk[:, -logits_to_keep-1:]\n",
    "                completion_input_ids = input_ids_chunk[:, -logits_to_keep:]\n",
    "                new_logits_chunk = torch.matmul(new_h_chunk_slice, lm_head.weight.t())[:, :-1, :]\n",
    "                old_logits_chunk = torch.matmul(old_h_chunk_slice, lm_head.weight.t())[:, :-1, :]\n",
    "                loss_chunk, _, _ = grpo_compute_loss(\n",
    "                    old_logits_chunk, new_logits_chunk, completion_input_ids, comp_mask_chunk, beta, adv_chunk\n",
    "                )\n",
    "            weight = (end_idx - i) / bsz\n",
    "            grad_inputs = torch.autograd.grad(loss_chunk, (old_h_chunk, new_h_chunk), grad_loss * weight, retain_graph=False)\n",
    "            grad_old[i:end_idx] = grad_inputs[0]\n",
    "            grad_new[i:end_idx] = grad_inputs[1]\n",
    "        # return two grads because two hidden states, since its grpo objective.\n",
    "        return grad_old, grad_new, None, None, None, None, None, None, None\n",
    "\n",
    "def memory_efficient_grpo_loss(old_hidden_states, new_hidden_states, lm_head, input_ids, logits_to_keep, completion_mask, beta, advantages, chunk_size=1):\n",
    "    return MemoryEfficientGRPO.apply(old_hidden_states, new_hidden_states, lm_head, input_ids, logits_to_keep, completion_mask, beta, advantages, chunk_size)\n",
    "\n",
    "def transformation_function_autogressive(batch, linear, labels):\n",
    "    logits = linear(batch).float()\n",
    "    shift_logits = logits[:, :-1, :]\n",
    "    shift_labels = labels[:, 1:]\n",
    "    return F.cross_entropy(shift_logits.view(-1, shift_logits.shape[-1]), shift_labels.view(-1))\n",
    "\n",
    "class BatchChunkedLlamaForCausalLM(LlamaPreTrainedModel, GenerationMixin):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n",
    "    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model = decoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model\n",
    "\n",
    "    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        **kwargs: Unpack[KwargsForCausalLM],\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n",
    "                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n",
    "                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n",
    "                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n",
    "                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n",
    "                This is useful when using packed tensor format (single dimension for batch and sequence length).\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "        ### my modification\n",
    "        loss = memory_efficient_linear(hidden_states[:, slice_indices, :], self.lm_head, labels=labels, forward_function=transformation_function_autogressive, chunk_size=1)\n",
    "        return loss\n",
    "\n",
    "old_hidden_states = torch.randn(batch_size, seq_len, hidden_dim, device=\"cuda\", requires_grad=True)\n",
    "new_hidden_states = torch.randn(batch_size, seq_len, hidden_dim, device=\"cuda\", requires_grad=True)\n",
    "lm_head = torch.nn.Linear(hidden_dim, vocab_size, bias=False, device=\"cuda\")\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=\"cuda\")\n",
    "logits_to_keep = seq_len-1\n",
    "completion_mask = torch.ones((batch_size, logits_to_keep), device=\"cuda\")\n",
    "beta = 0.04\n",
    "advantages = torch.randn(batch_size, device=\"cuda\")\n",
    "\n",
    "# VRAM benchmarking for GRPO\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "loss_std_grpo, comp_length_std_grpo, mean_kl_std_grpo = grpo_accumulated_loss(\n",
    "    old_hidden_states,\n",
    "    new_hidden_states,\n",
    "    lm_head,\n",
    "    input_ids,\n",
    "    logits_to_keep,\n",
    "    completion_mask,\n",
    "    beta,\n",
    "    advantages\n",
    ")\n",
    "loss_std_grpo.backward()\n",
    "std_mem_grpo = torch.cuda.max_memory_allocated(device)\n",
    "print(\"Standard GRPO Loss:\", loss_std_grpo.item())\n",
    "print(\"Standard GRPO max gpu memory: {:.2f} MB\".format(std_mem_grpo / (1024 * 1024)))\n",
    "\n",
    "lm_head.zero_grad()\n",
    "if old_hidden_states.grad is not None:\n",
    "    old_hidden_states.grad.zero_()\n",
    "if new_hidden_states.grad is not None:\n",
    "    new_hidden_states.grad.zero_()\n",
    "\n",
    "### Using MemoryEfficientGRPO with chunking on batch dimension\n",
    "chunk_size = 1\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "loss_chunked_grpo, comp_length_chunked_grpo, mean_kl_chunked_grpo = memory_efficient_grpo_loss(\n",
    "    old_hidden_states,\n",
    "    new_hidden_states,\n",
    "    lm_head,\n",
    "    input_ids,\n",
    "    logits_to_keep,\n",
    "    completion_mask,\n",
    "    beta,\n",
    "    advantages,\n",
    "    chunk_size=chunk_size\n",
    ")\n",
    "loss_chunked_grpo.backward()\n",
    "chunked_mem_grpo = torch.cuda.max_memory_allocated(device)\n",
    "print(\"Memory-Efficient GRPO Loss:\", loss_chunked_grpo.item())\n",
    "print(\"Memory-Efficient GRPO max gpu memory: {:.2f} MB\".format(chunked_mem_grpo / (1024 * 1024)))\n",
    "reduction_pct_grpo = (std_mem_grpo - chunked_mem_grpo) / std_mem_grpo * 100\n",
    "print(\"GRPO VRAM reduction: {:.1f}%\".format(reduction_pct_grpo))\n",
    "\n",
    "lm_head.zero_grad()\n",
    "if old_hidden_states.grad is not None:\n",
    "    old_hidden_states.grad.zero_()\n",
    "if new_hidden_states.grad is not None:\n",
    "    new_hidden_states.grad.zero_()\n",
    "\n",
    "# import config from real Llama model but init random\n",
    "config_real_llama3_2_1B = AutoConfig.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "chunked_llama = BatchChunkedLlamaForCausalLM(config_real_llama3_2_1B).to(device)\n",
    "classic_llama = LlamaForCausalLM(config_real_llama3_2_1B).to(device)\n",
    "\n",
    "chunked_llama.load_state_dict(classic_llama.state_dict())\n",
    "\n",
    "torch.manual_seed(0)\n",
    "input_ids = torch.randint(0, config_real_llama3_2_1B.vocab_size, (batch_size, seq_len)).to(device)\n",
    "labels = input_ids.clone()\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "outputs_classic = classic_llama(input_ids=input_ids, labels=labels)\n",
    "loss_classic = outputs_classic.loss\n",
    "loss_classic.backward()\n",
    "std_mem = torch.cuda.max_memory_allocated(device)\n",
    "print(\"Classic loss:\", loss_classic.item())\n",
    "print(\"Classic max gpu memory: {:.2f} MB\".format(std_mem / (1024 * 1024)))\n",
    "\n",
    "classic_llama.zero_grad()\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "loss_chunked = chunked_llama(input_ids=input_ids, labels=labels)\n",
    "loss_chunked.backward()\n",
    "chunked_mem = torch.cuda.max_memory_allocated(device)\n",
    "print(\"Chunked loss:\", loss_chunked.item())\n",
    "print(\"Chunked max gpu memory: {:.2f} MB\".format(chunked_mem / (1024 * 1024)))\n",
    "\n",
    "assert torch.allclose(loss_classic, loss_chunked), \"Losses do not match!\"\n",
    "\n",
    "reduction_pct = (std_mem - chunked_mem) / std_mem * 100\n",
    "print(\"VRAM reduction: {:.1f}%\".format(reduction_pct))\n",
    "\n",
    "# Full model vram reduction cannot be as large as %50 ish because we only chunk for the last lm_head and loss\n",
    "# so its expected, its still good if its %10+, it will get better on models like gemma where lm head has larger vocabulary sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1543a64e-b47b-454e-aa3d-684b7142ac63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
