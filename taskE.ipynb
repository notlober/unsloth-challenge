{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckcqLGw4rIKg",
        "outputId": "2dbaef37-eb36-4701-b153-5bc1c951e2c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with cross_entropy\n",
            "Forward pass matches: True\n",
            "Backward pass matches: True\n",
            "Testing with multi_margin_loss\n",
            "Forward pass matches: True\n",
            "Backward pass matches: True\n",
            "Original approach OOM as expected\n",
            "Memory efficient approach succeeded\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# i just found out\n",
        "# you dont need custom autograd function (e.g., a MemoryEfficientLinear class)\n",
        "# why we needed a custom autograd function was to let it generalize to other loss functions\n",
        "# in here i apply the same by accepting a loss_fn\n",
        "# so it will generalize to any loss function that accept input in same style\n",
        "def chunk_function(batch, linear, labels, loss_fn, n_chunks=1):\n",
        "    batch_chunks = torch.chunk(batch, n_chunks)\n",
        "    labels_chunks = torch.chunk(labels, n_chunks)\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_chunk, labels_chunk in zip(batch_chunks, labels_chunks):\n",
        "        x = linear(batch_chunk).float()\n",
        "        loss = loss_fn(x.view(-1, x.shape[-1]), labels_chunk.view(-1)) / n_chunks\n",
        "        total_loss += loss\n",
        "    return total_loss\n",
        "\n",
        "def test_small_case():\n",
        "    torch.manual_seed(0)\n",
        "    batch_size, seq_len, hidden_dim, vocab_size = 2, 4, 8, 16\n",
        "\n",
        "    X = torch.randn(batch_size, seq_len, hidden_dim, requires_grad=True)\n",
        "    labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "    linear = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    loss_fns = [F.cross_entropy, F.multi_margin_loss]\n",
        "\n",
        "    for loss_fn in loss_fns:\n",
        "        print(f\"Testing with {loss_fn.__name__}\")\n",
        "\n",
        "        out1 = loss_fn(linear(X).float().view(-1, vocab_size), labels.view(-1))\n",
        "        grad1 = torch.autograd.grad(out1, X)[0]\n",
        "\n",
        "        out2 = chunk_function(X, linear, labels, loss_fn)\n",
        "        grad2 = torch.autograd.grad(out2, X)[0]\n",
        "\n",
        "        print(\"Forward pass matches:\", torch.allclose(out1, out2))\n",
        "        print(\"Backward pass matches:\", torch.allclose(grad1, grad2))\n",
        "\n",
        "def test_large_case():\n",
        "    batch_size, seq_len, hidden_dim, vocab_size = 4, 4096, 4096, 128000\n",
        "\n",
        "    X = torch.randn(batch_size, seq_len, hidden_dim, requires_grad=True, device=\"cuda\")\n",
        "    labels = torch.randint(0, vocab_size, (batch_size, seq_len), device=\"cuda\")\n",
        "    linear = nn.Linear(hidden_dim, vocab_size).cuda()\n",
        "\n",
        "    try:\n",
        "        out1 = F.cross_entropy(linear(X).float().view(-1, vocab_size), labels.view(-1))\n",
        "        print(\"Original approach succeeded (unexpected)\")\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" in str(e):\n",
        "            print(\"Original approach OOM as expected\")\n",
        "\n",
        "    out2 = chunk_function(X, linear, labels, F.cross_entropy, 4)\n",
        "    print(\"Memory efficient approach succeeded\")\n",
        "\n",
        "test_small_case()\n",
        "test_large_case()\n",
        "\n",
        "# this will work with grpo and llama because nothing prevents it from that\n",
        "# also we validated the losses match already\n",
        "# and it generalizes to other loss functions already because it accepts a loss_fn\n",
        "# btw F.mse_loss and some other cases wont work since they accept different shapes\n",
        "# and even through you do custom autograd mse will still require modifications which prevents generalization"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
