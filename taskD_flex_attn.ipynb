{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab and Kaggle notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n    !pip install --no-deps cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer torch transformers torchvision -U\n    !pip install --no-deps git+https://github.com/notlober/unsloth.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T21:12:48.975967Z","iopub.execute_input":"2025-02-22T21:12:48.976257Z","iopub.status.idle":"2025-02-22T21:16:15.679594Z","shell.execute_reply.started":"2025-02-22T21:12:48.976235Z","shell.execute_reply":"2025-02-22T21:16:15.678712Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip uninstall unsloth -y\n!pip install --no-deps git+https://github.com/notlober/unsloth.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T21:16:15.680756Z","iopub.execute_input":"2025-02-22T21:16:15.680985Z","iopub.status.idle":"2025-02-22T21:16:26.213453Z","shell.execute_reply.started":"2025-02-22T21:16:15.680966Z","shell.execute_reply":"2025-02-22T21:16:26.212668Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: unsloth 2025.2.15\nUninstalling unsloth-2025.2.15:\n  Successfully uninstalled unsloth-2025.2.15\nCollecting git+https://github.com/notlober/unsloth.git\n  Cloning https://github.com/notlober/unsloth.git to /tmp/pip-req-build-6ift5_fm\n  Running command git clone --filter=blob:none --quiet https://github.com/notlober/unsloth.git /tmp/pip-req-build-6ift5_fm\n  Resolved https://github.com/notlober/unsloth.git to commit 3571a18479c9d6b58f9ec69970e69c0616b63821\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: unsloth\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for unsloth: filename=unsloth-2025.2.15-py3-none-any.whl size=188512 sha256=dcac13d960041c8d9b86d11a957827f736f761f4fb27a6252f22e1dab732a490\n  Stored in directory: /tmp/pip-ephem-wheel-cache-wygtivdz/wheels/79/fe/5b/4f72c49c4358fe03cb2f3e29b21f4f82f2e13e196fbc5e8f50\nSuccessfully built unsloth\nInstalling collected packages: unsloth\nSuccessfully installed unsloth-2025.2.15\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/gemma-2-9b-bnb-4bit\",\n    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n\n    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n\n    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T21:16:26.214834Z","iopub.execute_input":"2025-02-22T21:16:26.215149Z","iopub.status.idle":"2025-02-22T21:17:14.561397Z","shell.execute_reply.started":"2025-02-22T21:16:26.215117Z","shell.execute_reply":"2025-02-22T21:17:14.560725Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2c14db3db9647b593787a4c5651a726"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"893c3d9f1c2e496dacf528a4bc6daed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2141036f704041a786f20d29fbd4e07f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54dc007472c84fa4938bd677eef60594"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fe8df15586746d5af56aa7ea108d3e8"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T21:17:14.562546Z","iopub.execute_input":"2025-02-22T21:17:14.562812Z","iopub.status.idle":"2025-02-22T21:17:21.060927Z","shell.execute_reply.started":"2025-02-22T21:17:14.562791Z","shell.execute_reply":"2025-02-22T21:17:21.060137Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.2.15 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train[:1000]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T21:17:21.061821Z","iopub.execute_input":"2025-02-22T21:17:21.062086Z","iopub.status.idle":"2025-02-22T21:17:24.807250Z","shell.execute_reply.started":"2025-02-22T21:17:21.062064Z","shell.execute_reply":"2025-02-22T21:17:24.806569Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/982 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c18220000a4d448c9b6be8626d41db84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/117M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57454703aa994c6cab4e45ad08ff31b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a36f6373c7f439694e3e2f2d14d1ed0"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from unsloth.chat_templates import standardize_sharegpt\ndataset = standardize_sharegpt(dataset)\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T21:17:24.807966Z","iopub.execute_input":"2025-02-22T21:17:24.808193Z","iopub.status.idle":"2025-02-22T21:17:28.857633Z","shell.execute_reply.started":"2025-02-22T21:17:24.808163Z","shell.execute_reply":"2025-02-22T21:17:28.856987Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Standardizing format:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d55ef22fd2b4e35a3561bddef8b6a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1946d624b0df4f66b71abdc9afac58be"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        # num_train_epochs = 1, # Set this for 1 full training run.\n        max_steps = 10,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T21:19:34.492316Z","iopub.execute_input":"2025-02-22T21:19:34.492675Z","iopub.status.idle":"2025-02-22T21:19:40.343109Z","shell.execute_reply.started":"2025-02-22T21:19:34.492627Z","shell.execute_reply":"2025-02-22T21:19:40.342059Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d99334ce87f74547b4cb8d5c4574e30d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"509c2a3c0aaf4ac28ffaef780ddafbde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8167c12dd69b4b828116ee03834cb1d0"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T21:19:41.853602Z","iopub.execute_input":"2025-02-22T21:19:41.853964Z","iopub.status.idle":"2025-02-22T21:19:42.430515Z","shell.execute_reply.started":"2025-02-22T21:19:41.853933Z","shell.execute_reply":"2025-02-22T21:19:42.429888Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03d55383f7134e0d990ba4c4410b1ba8"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T21:19:44.531313Z","iopub.execute_input":"2025-02-22T21:19:44.531600Z","iopub.status.idle":"2025-02-22T21:22:15.199750Z","shell.execute_reply.started":"2025-02-22T21:19:44.531578Z","shell.execute_reply":"2025-02-22T21:22:15.199022Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 1,000 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 10\n \"-____-\"     Number of trainable parameters = 24,313,856\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 02:14, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.809800</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.882500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.922100</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.828600</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.719000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.563800</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.737300</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.651300</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.526500</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.921700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\noutputs = model.generate(input_ids = inputs, max_new_tokens = 256, use_cache = True,\n                         temperature = 1.5, min_p = 0.1)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T21:22:46.975922Z","iopub.execute_input":"2025-02-22T21:22:46.976277Z","iopub.status.idle":"2025-02-22T21:22:52.503764Z","shell.execute_reply.started":"2025-02-22T21:22:46.976250Z","shell.execute_reply":"2025-02-22T21:22:52.502918Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContinue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe Fibonacci sequence continues with the following terms:\\n\\n- 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144\\n\\nThe sequence is generated by adding the two previous numbers to find the next number in the sequence. The formula for the Fibonacci sequence is:\\n\\nF(n) = F(n-1) + F(n-2)\\n\\nwhere F(n) is the nth term of the sequence.<|eot_id|>']"},"metadata":{}}],"execution_count":14}]}