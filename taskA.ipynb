{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "SIWhq3-AeS0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "### WARNING: MODIFIED RTOL & ATOL\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True, atol=1e-4, rtol=1e-3)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ],
      "metadata": {
        "id": "c8GyUBUneUrk"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias = None,\n",
        "        compute_dtype       = dtype,\n",
        "        compress_statistics = True,\n",
        "        quant_type          = \"nf4\",\n",
        "    )\n",
        "\n",
        "# [NEW] as at 18th Feb 2025\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.dtype == dtype)\n",
        "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.blocksize == 64)\n",
        "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
        "        # [NEW] as at 18th Feb 2025\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj  .weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.  up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    elapsed = 0\n",
        "    options = [\n",
        "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "        (5,  777, 1024,  4096, 3409, torch.float16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.float16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            # [NEW] as at 18th Feb 2025\n",
        "            assert_correct_bnb(mlp.  up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmarking\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed"
      ],
      "metadata": {
        "id": "sdWs14cSeVvR"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "vIxSW_7S1ae4"
      },
      "outputs": [],
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "import math\n",
        "\n",
        "@triton.jit\n",
        "def dequantize_nf4_kernel_with_ptx(\n",
        "    q_ptr, absmax_ptr, code2_ptr, absmax2_ptr, nf4_table_ptr, out_ptr,\n",
        "    offset: float,\n",
        "    n_elements: tl.constexpr,\n",
        "    blocksize_log2: tl.constexpr,\n",
        "    blocksize2_log2: tl.constexpr,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    tid = tl.arange(0, BLOCK_SIZE)\n",
        "\n",
        "    # cant use fma here, block size is constexpr\n",
        "    elem_idx = pid * BLOCK_SIZE + tid\n",
        "    mask = elem_idx < n_elements\n",
        "\n",
        "    byte_idx = elem_idx >> 1\n",
        "    is_high_nibble = (elem_idx & 1) == 0\n",
        "\n",
        "    q_byte = tl.load(q_ptr + byte_idx, mask=mask)\n",
        "\n",
        "    nibble = tl.where(is_high_nibble, (q_byte >> 4) & 0xF, q_byte & 0xF)\n",
        "\n",
        "    block_idx = elem_idx >> blocksize_log2\n",
        "    block2_idx = block_idx >> blocksize2_log2\n",
        "\n",
        "    absmax_idx = tl.load(absmax_ptr + block_idx, mask=mask).to(tl.int32)\n",
        "\n",
        "    scale1 = tl.load(code2_ptr + absmax_idx, mask=mask)\n",
        "    scale2 = tl.load(absmax2_ptr + block2_idx, mask=mask)\n",
        "\n",
        "    # ptx assembly for fma\n",
        "    # tl.fma()\n",
        "    #\n",
        "    final_scale = tl.inline_asm_elementwise(\n",
        "        \"\"\"fma.rn.f32 $0, $1, $2, $3;\"\"\",\n",
        "        \"=f,f,f,f\",\n",
        "        [scale1, scale2, offset],\n",
        "        dtype=tl.float32,\n",
        "        is_pure=True,\n",
        "        pack=1\n",
        "    )\n",
        "\n",
        "    nf4_val = tl.load(nf4_table_ptr + nibble, mask=mask)\n",
        "\n",
        "    result = nf4_val * final_scale\n",
        "\n",
        "    # cache eviction, write once mode\n",
        "    # Use ld.cg to cache loads only globally, bypassing the L1 cache, and cache only in the L2 cache.\n",
        "    tl.store(out_ptr + elem_idx, result, mask=mask, cache_modifier='.cs')\n",
        "\n",
        "def my_dequantize_triton(weight):\n",
        "    q_data = weight.weight.data.view(-1)\n",
        "    qs = weight.weight.quant_state\n",
        "\n",
        "    n_elements = weight.out_features * weight.in_features\n",
        "    blocksize = qs.blocksize\n",
        "    blocksize2 = qs.state2.blocksize\n",
        "    nf4_table = qs.code\n",
        "    absmax = qs.absmax\n",
        "    code2 = qs.state2.code\n",
        "    absmax2 = qs.state2.absmax\n",
        "\n",
        "    offset = qs.offset.item()\n",
        "    output = torch.empty(n_elements, device=q_data.device, dtype=qs.dtype)\n",
        "\n",
        "    BLOCK_SIZE = 1024\n",
        "\n",
        "    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n",
        "\n",
        "    blocksize_log2 = int(math.log2(blocksize))\n",
        "    blocksize2_log2 = int(math.log2(blocksize2))\n",
        "\n",
        "    dequantize_nf4_kernel_with_ptx[grid](\n",
        "        q_data, absmax, code2, absmax2, nf4_table, output,\n",
        "        offset,\n",
        "        n_elements,\n",
        "        blocksize_log2, blocksize2_log2,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "    )\n",
        "    return output.view(weight.out_features, weight.in_features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dequantize(unsloth_dequantize) # This is the unsloth one"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziL-nTG4lpcN",
        "outputId": "27dec586-cda0-4c72-fa40-9fc39f2caacc"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.173987150192261"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dequantize(my_dequantize_triton) # This is the triton one"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouxF6qTTeYYR",
        "outputId": "843ba4b5-68e2-4260-88b5-101e3d5acbe0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.150385141372681"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "test_dequantize(unsloth_dequantize) / test_dequantize(my_dequantize_triton)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11UELOL6eXSh",
        "outputId": "750f443a-d5be-4feb-81ef-476d04a9f75e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0943343654188828"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### torch.compile versions below"
      ],
      "metadata": {
        "id": "GqaLnwW3rwXS"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.compile(fullgraph=True)\n",
        "def wrapper(weight):\n",
        "    return my_dequantize_triton(weight)"
      ],
      "metadata": {
        "id": "l2r6PA5wryWX"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dequantize(unsloth_dequantize) # This is the unsloth one"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asqTjhILr2yh",
        "outputId": "57677e1e-19b2-46f0-f27d-0f8408f84071"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.805891752243042"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dequantize(my_dequantize_triton) # This is the triton with torch compile one"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGv8IwQyr23q",
        "outputId": "fec7aa6d-3c8e-4294-8c1a-0ee7699cef8b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.695971965789795"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "test_dequantize(unsloth_dequantize) / test_dequantize(my_dequantize_triton)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnmPen0er26R",
        "outputId": "3ead30a0-0366-4919-e4ca-1656c70ba46d"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.3237344333580716"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    }
  ]
}