{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIWhq3-AeS0B"
      },
      "outputs": [],
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c8GyUBUneUrk"
      },
      "outputs": [],
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "### WARNING: MODIFIED RTOL & ATOL\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True, atol=1e-4, rtol=1e-3)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdWs14cSeVvR",
        "outputId": "ffccf7ca-9535-4354-b960-f231fb30c5b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
            "    Python  3.11.11 (you have 3.11.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias = None,\n",
        "        compute_dtype       = dtype,\n",
        "        compress_statistics = True,\n",
        "        quant_type          = \"nf4\",\n",
        "    )\n",
        "\n",
        "# [NEW] as at 18th Feb 2025\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.dtype == dtype)\n",
        "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.blocksize == 64)\n",
        "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
        "        # [NEW] as at 18th Feb 2025\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj  .weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.  up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    elapsed = 0\n",
        "    options = [\n",
        "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "        (5,  777, 1024,  4096, 3409, torch.float16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.float16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            # [NEW] as at 18th Feb 2025\n",
        "            assert_correct_bnb(mlp.  up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmarking\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vIxSW_7S1ae4"
      },
      "outputs": [],
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "import math\n",
        "\n",
        "@triton.jit\n",
        "def dequantize_nf4_kernel_with_ptx(\n",
        "    q_ptr, absmax_ptr, code2_ptr, absmax2_ptr, nf4_table_ptr, out_ptr,\n",
        "    offset: float,\n",
        "    n_elements: tl.constexpr,\n",
        "    blocksize_log2: tl.constexpr,\n",
        "    blocksize2_log2: tl.constexpr,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    tid = tl.arange(0, BLOCK_SIZE)\n",
        "\n",
        "    # cant use fma here, block size is constexpr\n",
        "    elem_idx = pid * BLOCK_SIZE + tid\n",
        "    mask = elem_idx < n_elements\n",
        "\n",
        "    byte_idx = elem_idx >> 1\n",
        "    is_high_nibble = (elem_idx & 1) == 0\n",
        "\n",
        "    q_byte = tl.load(q_ptr + byte_idx, mask=mask)\n",
        "\n",
        "    nibble = tl.where(is_high_nibble, (q_byte >> 4) & 0xF, q_byte & 0xF)\n",
        "\n",
        "    block_idx = elem_idx >> blocksize_log2\n",
        "    block2_idx = block_idx >> blocksize2_log2\n",
        "\n",
        "    absmax_idx = tl.load(absmax_ptr + block_idx, mask=mask).to(tl.int32)\n",
        "\n",
        "    scale1 = tl.load(code2_ptr + absmax_idx, mask=mask)\n",
        "    scale2 = tl.load(absmax2_ptr + block2_idx, mask=mask)\n",
        "\n",
        "    # ptx assembly for fma\n",
        "    # tl.fma()\n",
        "    #\n",
        "    final_scale = tl.inline_asm_elementwise(\n",
        "        \"\"\"fma.rn.f32 $0, $1, $2, $3;\"\"\",\n",
        "        \"=f,f,f,f\",\n",
        "        [scale1, scale2, offset],\n",
        "        dtype=tl.float32,\n",
        "        is_pure=True,\n",
        "        pack=1\n",
        "    )\n",
        "\n",
        "    nf4_val = tl.load(nf4_table_ptr + nibble, mask=mask)\n",
        "\n",
        "    result = nf4_val * final_scale\n",
        "\n",
        "    # cache eviction, write once mode\n",
        "    # Use ld.cg to cache loads only globally, bypassing the L1 cache, and cache only in the L2 cache.\n",
        "    tl.store(out_ptr + elem_idx, result, mask=mask, cache_modifier='.cs')\n",
        "\n",
        "def my_dequantize_triton(weight):\n",
        "    q_data = weight.weight.data.view(-1)\n",
        "    qs = weight.weight.quant_state\n",
        "\n",
        "    n_elements = weight.out_features * weight.in_features\n",
        "    blocksize = qs.blocksize\n",
        "    blocksize2 = qs.state2.blocksize\n",
        "    nf4_table = qs.code\n",
        "    absmax = qs.absmax\n",
        "    code2 = qs.state2.code\n",
        "    absmax2 = qs.state2.absmax\n",
        "\n",
        "    offset = qs.offset.item()\n",
        "    output = torch.empty(n_elements, device=q_data.device, dtype=qs.dtype)\n",
        "\n",
        "    BLOCK_SIZE = 1024\n",
        "\n",
        "    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n",
        "\n",
        "    blocksize_log2 = int(math.log2(blocksize))\n",
        "    blocksize2_log2 = int(math.log2(blocksize2))\n",
        "\n",
        "    dequantize_nf4_kernel_with_ptx[grid](\n",
        "        q_data, absmax, code2, absmax2, nf4_table, output,\n",
        "        offset,\n",
        "        n_elements,\n",
        "        blocksize_log2, blocksize2_log2,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "    )\n",
        "    return output.view(weight.out_features, weight.in_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziL-nTG4lpcN",
        "outputId": "c3ff7570-121e-4df5-a175-dbb4556c025b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5.146984577178955"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dequantize(unsloth_dequantize) # This is the unsloth one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouxF6qTTeYYR",
        "outputId": "da84d627-2fd8-4499-b937-06693e5e4dfa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.869208812713623"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dequantize(my_dequantize_triton) # This is the triton one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11UELOL6eXSh",
        "outputId": "1559b4f0-c487-4d20-b48d-7418bb5f2f23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.133116476394387"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "test_dequantize(unsloth_dequantize) / test_dequantize(my_dequantize_triton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GqaLnwW3rwXS",
        "outputId": "478d0b58-606e-4284-a22c-9f7b637fe8f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.6.0\n",
            "Uninstalling torch-2.6.0:\n",
            "  Successfully uninstalled torch-2.6.0\n",
            "Collecting torch\n",
            "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting triton==3.2.0 (from torch)\n",
            "  Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torchvision\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~riton (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: torchvision 0.20.1+cu124\n",
            "    Uninstalling torchvision-0.20.1+cu124:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu124\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~riton (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~riton (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~riton (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2025.2.15 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.2.7 requires tyro, which is not installed.\n",
            "unsloth 2025.2.15 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\n",
            "xformers 0.0.29 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "unsloth-zoo 2025.2.7 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.6.0 torchvision-0.21.0 triton\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "575f8402fd6a4f7981e70df814bd1af3",
              "pip_warning": {
                "packages": [
                  "functorch",
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### torch.compile versions below\n",
        "# to fix a torch compile problem\n",
        "!pip uninstall torch -y\n",
        "!pip install torch torchvision --pre --upgrade\n",
        "# restart kernel but do not run this block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOAHkusAwzLI"
      },
      "outputs": [],
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "import math\n",
        "\n",
        "@triton.jit\n",
        "def dequantize_nf4_kernel_torch_compile(\n",
        "    q_ptr, absmax_ptr, code2_ptr, absmax2_ptr, nf4_table_ptr, out_ptr,\n",
        "    offset: float,\n",
        "    n_elements: tl.constexpr,\n",
        "    blocksize_log2: tl.constexpr,\n",
        "    blocksize2_log2: tl.constexpr,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    tid = tl.arange(0, BLOCK_SIZE)\n",
        "\n",
        "    elem_idx = pid * BLOCK_SIZE + tid\n",
        "    mask = elem_idx < n_elements\n",
        "\n",
        "    byte_idx = elem_idx >> 1\n",
        "    is_high_nibble = (elem_idx & 1) == 0\n",
        "\n",
        "    q_byte = tl.load(q_ptr + byte_idx, mask=mask)\n",
        "\n",
        "    nibble = tl.where(is_high_nibble, (q_byte >> 4) & 0xF, q_byte & 0xF)\n",
        "\n",
        "    block_idx = elem_idx >> blocksize_log2\n",
        "    block2_idx = block_idx >> blocksize2_log2\n",
        "\n",
        "    absmax_idx = tl.load(absmax_ptr + block_idx, mask=mask).to(tl.int32)\n",
        "\n",
        "    scale1 = tl.load(code2_ptr + absmax_idx, mask=mask)\n",
        "    scale2 = tl.load(absmax2_ptr + block2_idx, mask=mask)\n",
        "\n",
        "    # do not use ptx for torch compile\n",
        "    # do not use fma for torch compile\n",
        "    final_scale = scale1 * scale2 + offset\n",
        "\n",
        "    nf4_val = tl.load(nf4_table_ptr + nibble, mask=mask)\n",
        "\n",
        "    result = nf4_val * final_scale\n",
        "\n",
        "    # do not use cache eviction for torch compile\n",
        "    tl.store(out_ptr + elem_idx, result, mask=mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l2r6PA5wryWX"
      },
      "outputs": [],
      "source": [
        "@torch.compile(fullgraph=True)\n",
        "def compiled_kernel(q_data, absmax, code2, absmax2, nf4_table,\n",
        "                  offset, n_elements, blocksize, blocksize2, dtype):\n",
        "    output = torch.empty(n_elements, device=q_data.device, dtype=dtype)\n",
        "\n",
        "    BLOCK_SIZE = 1024\n",
        "    blocksize_log2 = int(math.log2(blocksize))\n",
        "    blocksize2_log2 = int(math.log2(blocksize2))\n",
        "\n",
        "    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n",
        "\n",
        "    dequantize_nf4_kernel_torch_compile[grid](\n",
        "        q_data, absmax, code2, absmax2, nf4_table, output,\n",
        "        offset,\n",
        "        n_elements,\n",
        "        blocksize_log2, blocksize2_log2,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "    )\n",
        "    return output\n",
        "\n",
        "def my_dequantize_triton_torch_compile(weight):\n",
        "    q_data = weight.weight.data.view(-1)\n",
        "    qs = weight.weight.quant_state\n",
        "\n",
        "    n_elements = weight.out_features * weight.in_features\n",
        "    blocksize = qs.blocksize\n",
        "    blocksize2 = qs.state2.blocksize\n",
        "    nf4_table = qs.code\n",
        "    absmax = qs.absmax\n",
        "    code2 = qs.state2.code\n",
        "    absmax2 = qs.state2.absmax\n",
        "\n",
        "    offset = qs.offset.item()\n",
        "\n",
        "    output = compiled_kernel(q_data, absmax, code2, absmax2, nf4_table,\n",
        "                            offset, n_elements, blocksize, blocksize2, qs.dtype)\n",
        "\n",
        "    return output.view(weight.out_features, weight.in_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asqTjhILr2yh",
        "outputId": "87a87c78-13f5-4062-b9c3-ce1fb7c3df46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5.694186687469482"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dequantize(unsloth_dequantize) # This is the unsloth one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGv8IwQyr23q",
        "outputId": "5534d5a8-ffe4-4674-b3a6-eadaf677a436"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5.683187484741211"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dequantize(my_dequantize_triton_torch_compile) # This is the triton with torch compile one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnmPen0er26R",
        "outputId": "5dc59779-34f1-45ae-ec76-11f8182a9b44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0133595462093137"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "test_dequantize(unsloth_dequantize) / test_dequantize(my_dequantize_triton_torch_compile)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
